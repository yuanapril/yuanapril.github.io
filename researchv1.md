## Publications

---

<div class="publication-item">
  <div class="pub-image">
    <img src="imgs/st-vad.png" alt="ST-VAD Teaser">
    <div class="pub-tag">IJCAI 2026</div>
  </div>
  <div class="pub-content">
    <h3>ST-VAD: Spatial-Temporal Mental Modeling for Industrial Video Anomaly Detection via Object-Centric Reasoning</h3>
    <p class="pub-authors"><strong>Mei Yuan</strong>, Yang Liu, Min Xu</p>
    <p class="pub-venue">Submitted to IJCAI 2026</p>
    <div class="pub-links">
      <a href="#" class="pub-link">üìÑ Paper</a>
      <a href="#" class="pub-link">üìä Project</a>
    </div>
    <details class="pub-tldr-collapsible">
      <summary><strong>TL;DR</strong></summary>
      <p class="tldr-content">A VLM-based reasoning framework that elevates video anomaly detection from pattern matching to cognitive-level understanding. By simulating human spatial perception and representing scene dynamics via object-centric state graphs, our approach achieves state-of-the-art performance on industrial benchmarks, pioneering explainable anomaly detection for robotic laboratories.</p>
    </details>
    <details class="pub-details">
      <summary>More details</summary>
      <p>We propose a framework that bridges low-level perception with high-level reasoning through three key innovations: (1) multi-view interpolation for spatial perception, (2) object-centric state subgraphs for scene dynamics representation, and (3) dense spatiotemporal rewards for learning. This work demonstrates that cognitive understanding‚Äîrather than mere pattern matching‚Äîis essential for trustworthy AI-driven quality control in manufacturing and automated laboratories.</p>
    </details>
  </div>
</div>

---

<div class="publication-item">
  <div class="pub-image">
    <img src="imgs/time-star.png" alt="Time-STaR Framework">
    <div class="pub-tag">ACL 2026</div>
  </div>
  <div class="pub-content">
    <h3>Time-STaR: Self-Taught Reasoners Augmented with Tools for Reliable Time Series Analysis</h3>
    <p class="pub-authors"><strong>Mei Yuan</strong>, Chang Xu, Jiang Bian</p>
    <p class="pub-venue">Submitted to ACL 2026</p>
    <div class="pub-links">
      <a href="#" class="pub-link">üìÑ Paper</a>
      <a href="#" class="pub-link">üíª Code</a>
    </div>
    <details class="pub-tldr-collapsible">
      <summary><strong>TL;DR</strong></summary>
      <p class="tldr-content">A reasoning-centric framework that repurposes LLMs for time series forecasting. By curating the Time-STaR-CoTT dataset and implementing GRPO-style reinforcement learning, we enable models to identify causal relationships, detect regime changes, and generate interpretable forecasts‚Äîachieving state-of-the-art results across weather, traffic, and finance domains.</p>
    </details>
    <details class="pub-details">
      <summary>More details</summary>
      <p>Traditional time series models excel at pattern matching but lack temporal reasoning capabilities. Time-STaR addresses this by training LLMs to understand why patterns emerge, not just predict them. Our framework features: (1) structured reasoning trajectories capturing cognitive patterns, (2) adaptive length penalties for efficient reasoning, and (3) dynamic tool interaction for domain-specific analysis. Results demonstrate that temporal intelligence requires understanding causality and detecting when dynamics shift.</p>
    </details>
  </div>
</div>

---

<div class="publication-item">
  <div class="pub-image">
    <img src="imgs/pronunciation.png" alt="Pronunciation Coaching System">
    <div class="pub-tag">CHI 2026</div>
  </div>
  <div class="pub-content">
    <h3>Guiding Grasp and Growth: Multi-Modal Detection and Feedback on Accented Mispronunciation</h3>
    <p class="pub-authors"><strong>Mei Yuan</strong>, Boting Li</p>
    <p class="pub-venue">Submitted to CHI 2026</p>
    <div class="pub-links">
      <a href="https://drive.google.com/file/d/1Cld1n7yeURCJH_tsa8Z_pG_sF3j9kN73/view?usp=drive_link" class="pub-link">üìÑ Paper</a>
      <a href="#" class="pub-link">üéØ Demo</a>
    </div>
    <details class="pub-tldr-collapsible">
      <summary><strong>TL;DR</strong></summary>
      <p class="tldr-content">An interactive text-vision-audio pronunciation coaching system combining LLM-powered assessment, Neural TTS exemplars, and viseme animations. Validated with 82 students showing 90%+ satisfaction, the system was adopted as an intelligent teaching assistant in a graduate-level English course at Peking University.</p>
    </details>
    <details class="pub-details">
      <summary>More details</summary>
      <p>Effective language learning requires multimodal feedback‚Äîtext alone cannot capture pronunciation nuances. Our system provides comprehensive, contextualized instruction through three modalities: (1) LLM-powered mispronunciation detection and diagnostic feedback, (2) Neural TTS generating native-speaker exemplars, and (3) synchronized viseme animations for articulatory guidance. Extensive evaluations demonstrate significant accent improvement, revealing that multimodal feedback loops create deeper understanding than single-modality approaches.</p>
    </details>
  </div>
</div>

---

<div class="publication-item">
  <div class="pub-image">
    <img src="imgs/prformer.png" alt="PRformer Architecture">
    <div class="pub-tag">IJCNN 2026</div>
  </div>
  <div class="pub-content">
    <h3>PRformer: Prefix Reprogramming Transformers for Long-Term Series Forecasting</h3>
    <p class="pub-authors"><strong>Mei Yuan</strong>, Hongcheng Guo</p>
    <p class="pub-venue">Submitted to IJCNN 2026</p>
    <div class="pub-links">
      <a href="#" class="pub-link">üìÑ Paper</a>
      <a href="#" class="pub-link">üíª Code</a>
    </div>
    <details class="pub-tldr-collapsible">
      <summary><strong>TL;DR</strong></summary>
      <p class="tldr-content">A novel approach for long-term time-series forecasting through Prefix Reprogramming with noise-based prefixes, FFT-Attention for periodic patterns, and Average Decomposition for seasonal-trend separation‚Äîavoiding information bottlenecks common in traditional Transformer architectures.</p>
    </details>
    <details class="pub-details">
      <summary>More details</summary>
      <p>Long-term forecasting challenges Transformers due to information bottlenecks when processing extended sequences. PRformer addresses this through: (1) prefix reprogramming that injects learnable noise-based prefixes to enhance representation capacity, (2) FFT-Attention modules extracting frequency-domain periodic patterns efficiently, and (3) average decomposition mechanism separating seasonal and trend components without losing information. Our method demonstrates improved performance across multiple domains for long-horizon forecasting tasks.</p>
    </details>
  </div>
</div>

---

<div class="publication-item">
  <div class="pub-image">
    <img src="imgs/diffuvst.png" alt="DiffuVST Visual Storytelling">
    <div class="pub-tag">EMNLP 2023</div>
  </div>
  <div class="pub-content">
    <h3>DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models</h3>
    <p class="pub-authors">Shengguang Wu, <strong>Mei Yuan</strong>, Qi Su</p>
    <p class="pub-venue">Findings of EMNLP 2023</p>
    <div class="pub-links">
      <a href="https://arxiv.org/pdf/2312.07066v1" class="pub-link">üìÑ Paper</a>
      <a href="#" class="pub-link">üåê Project</a>
    </div>
    <details class="pub-tldr-collapsible">
      <summary><strong>TL;DR</strong></summary>
      <p class="tldr-content">A non-autoregressive DiffusionLM-based storytelling model that generates coherent narratives around visual sequences. Trained with weighted conditions on global vision-language history, DiffuVST achieves superior performance with 10√ó faster inference than autoregressive models.</p>
    </details>
    <details class="pub-details">
      <summary>More details</summary>
      <p>Traditional autoregressive models generate stories sequentially, limiting narrative coherence and inference speed. DiffuVST reframes visual storytelling as diffusion-based parallel generation, conditioning on the global history of vision-language features across the entire visual stream. This holistic context understanding enables: (1) maintaining narrative coherency across long sequences, (2) capturing visual-linguistic interactions more effectively, and (3) achieving significantly faster generation speeds. Results demonstrate that parallel generation with global context surpasses sequential approaches for creative tasks.</p>
    </details>
  </div>
</div>

---

<div class="publication-item">
  <div class="pub-image">
    <img src="imgs/person-reid.png" alt="Person Re-identification">
    <div class="pub-tag">ACM MM 2022</div>
  </div>
  <div class="pub-content">
    <h3>A Person Re-identification Approach Focusing on the Occlusion Problem and Ranking Optimization</h3>
    <p class="pub-authors">Wenkai Zheng, <strong>Mei Yuan</strong></p>
    <p class="pub-venue">ACM Multimedia 2022 (MMSports Workshop)</p>
    <div class="pub-links">
      <a href="https://dl.acm.org/doi/abs/10.1145/3552437.3555692" class="pub-link">üìÑ Paper</a>
      <a href="#" class="pub-link">üèÜ 2nd Place</a>
    </div>
    <details class="pub-tldr-collapsible">
      <summary><strong>TL;DR</strong></summary>
      <p class="tldr-content">A robust person re-identification method addressing occlusion challenges through dual-branch Vision Transformer with jigsaw patch modules and innovative ranking optimization. Achieved 98.38% mAP and 99.57% rank-1 accuracy, winning second place in the DeepSportRadar Player REID Challenge.</p>
    </details>
    <details class="pub-details">
      <summary>More details</summary>
      <p>Sports video analysis faces significant occlusion challenges that hinder player tracking. Our approach combines: (1) dual-branch feature extraction with Vision Transformers enhanced by jigsaw patch modules for robust local features under occlusion, (2) k-reciprocal re-ranking for improving retrieval accuracy, (3) metric fusion combining multiple distance measures, and (4) distance mapping for refined similarity computation. This comprehensive framework demonstrates that geometric robustness and ranking refinement are essential for reliable tracking in complex visual scenarios.</p>
    </details>
  </div>
</div>

---

### Also see [Google Scholar](https://scholar.google.com/citations?user=q4kZ8WMAAAAJ&hl=en) for the complete list.